# RuleText-AD: Logical Anomaly Detection via Textual Rule Engines Generated by MLLMs

This repository contains the code for the paper **‚ÄúRuleText-AD: Logical Anomaly Detection via Textual Rule Engines Generated by MLLMs.‚Äù**

RuleText-AD is a lightweight, two-phase pipeline for **logical anomaly detection** that (1) induces **human-readable rules** from a few normal examples and (2) checks new images against those rules using MLLMs‚Äî**no external segmenters, no heavy vision backbones**. The implementation supports multiple model providers behind a unified interface.

---

## üìÑ Links

- **Paper (PDF):**  
  `https://`
- **Supplementary Material (ZIP):**  
  `https://`

---

## üß∞ Environment Setup (via `requirements.txt`)

Use a Python virtual environment and install dependencies from `requirements.txt`.

**1) Create & activate a virtual environment**

On **Linux/macOS**:
~~~bash
python3 -m venv .venv
source .venv/bin/activate
~~~

**2) Install dependencies**
~~~bash
python -m pip install --upgrade pip
pip install -r requirements.txt
~~~

---

## üîê Secrets & Keys ‚Äî Creating your `.env`

The project reads provider credentials from a `.env` file at the **repository root**.  
Create a new file named `.env` with only the sections you need:

~~~dotenv
# Default provider (can also be set in your .config YAML)
# Options: vertexai | azure | openai | groq | openrouter
PROVIDER=vertexai

##############################
# Google Vertex AI (Gemini 2.x)
##############################
# Service account auth:
GOOGLE_APPLICATION_CREDENTIALS=/absolute/path/to/service-account.json
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_CLOUD_LOCATION=us-central1

# Or, if using the API key flow (Google AI Studio):
# GOOGLE_GENAI_KEY=xxxxxxxxxxxxxxxxxxxxxxxx

#####################################
# Azure OpenAI (e.g., GPT-4.1, o4-mini)
#####################################
AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxx
AZURE_OPENAI_ENDPOINT=https://<your-resource-name>.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-15-preview

##############################
# OpenAI (direct)
##############################
OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxx

##############################
# Groq (e.g., Llama 4 Maverick)
##############################
GROQ_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxx

##############################
# OpenRouter (optional)
##############################
OPENROUTER_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxx
~~~

**Notes**
- If the code uses `python-dotenv`, `.env` will be auto-loaded. Otherwise, export variables in your shell before running.
- Keep secrets out of commits and CI logs.

---

## ‚öôÔ∏è Experiments via `.config` (YAML)

Experiments are configured with YAML files (we recommend storing them under a `.config/` folder).  
Here‚Äôs a minimal example:

~~~yaml
# .config/example.loco.gemini25.yaml
datasets:
    -
        name: mvtec_loco_anomaly_detection
        path: /absolute/path/to/MVTec-LOCO

mvtec_loco_objects:
    -
        name: <object name>
        train: [<list of images used as k-shot samples>]

models:
    -
        provider: <provider name>
        llm: <name of your LLM>
        litellm: <identification on the provider>
        rpm: <maximum requests per minute>
        rpd: <maximum requests per day>
        temperature: <temperature>
        max_tokens: <maximum of tokens>
        seed: <seed value>
~~~

See the config example in the repository for more examples.

---

## üöÄ Running the Main Script

Use **`run_logical.py`** as the entry point.

~~~bash
python run_logical.py
~~~

**What happens**
1. **Rule definition (few-shot):** Reads the list of images used as k-shot normal images and induces a compact, human-readable rule set.
2. **Testing:** Parses candidate images with the same schema; any rule violations flag logical anomalies.
3. **Outputs:** Saves the learned rule engine, predictions, and logs under `./results`.

---

## üß™ Repro Tips

- Set `temperature: 0.0 ~ 0.3` and a fixed `seed` for determinism.
- Reasoning-capable models may yield higher F1 on spatial-logic classes; lighter models are faster/cheaper.
- Keep the list of images used as k-shot high enough to capture layout variability across normal samples.


---

## üìö Citation

If you use this code, please cite:

~~~bibtex
@inproceedings{[TODO],
  title     = {[TODO]},
  author    = {[TODO]},
  booktitle = {[TODO] },
  year      = {2025},
  url       = {[TODO]},
}
~~~

---
